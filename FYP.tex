\documentclass[british, twoside]{bhamthesis}
\title{Applying Static Type Checking to JavaScript}
\author{Jack Wearden}
\supervisor{Hayo Thielecke}
\degree{Bachelor of Science}
\department{Computer Science}
\date{April 2016}  %% Version 2009/12/26

\usepackage{listings}
\lstset{
  basicstyle=\ttfamily\scriptsize,
  breaklines=true,
  postbreak=\raisebox{0ex}[0ex][0ex]{\ensuremath{\hookrightarrow\space}}
}
\usepackage{babel}
\usepackage{csquotes}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsmath,array,booktabs}
\usepackage[sorting=nyt,style=apa]{biblatex}
\DeclareLanguageMapping{british}{british-apa}
\addbibresource{bhamthesis.bib}

\newtheorem*{thm}{Theorem}

\theoremstyle{definition}
\newtheorem*{defn}{Definition}

\newcommand{\mar}[1]{\marginpar{\raggedright#1}}
\newcommand{\clsname}{\textsf{bhamthesis}}
\newcommand{\bktitle}[1]{\textit{#1}}
\newcommand{\ZF}{\mathrm{ZF}}
\newcommand{\IN}{\mathbb{N}}

\newcommand{\hmcolon}{{\mspace{2mu}:\mspace{2mu}}}

\makeatletter
\newcommand{\Cen}[2]{%
  \ifmeasuring@
    #2%
  \else
    \makebox[\ifcase\expandafter #1\maxcolumn@widths\fi]{$\displaystyle#2$}%
  \fi
}
\makeatother

\lstdefinelanguage{JavaScript}%
 {morekeywords={function, return, var, if, else, for, while, continue, break};%
   }[keywords,comments,strings]


\begin{document}
\maketitle

\begin{abstract}
\end{abstract}

\tableofcontents

\chapter{Introduction}

\chapter{Background}

  \section{JavaScript}
    Created in 1995 by Brendan Eich, JavaScript aimed to enhance the existing capabilities of HTML and CSS by allowing developers to program custom behaviour on pages viewed in Netscape Navigator.

    Today, JavaScript is the only programming language supported by all major browsers, and as a result has a huge footprint in the technology industry. As well as having dominance on the desktop and mobile web, it's increasingly used for server side and desktop applications through Node.js.

    Though it’s C-like syntax and presence of “Java” in the name implies it might be an imperative or object-oriented language, it’s origins really derive from from Self, a Smalltalk dialect. JavaScript has closures and first class functions, and due to being predicated on running in a single-threaded environment, it makes heavy use of those features in conjunction with asynchronous programming to manage blocking operations.

    As well as having dynamic types, JavaScript makes use of implicit type coercion, often causing unexpected results.

    \begin{lstlisting}[language=JavaScript]
      0 + "" == "0"
      "0" - 1 == -1
      "" - "" == 0
      {} + [] == 0
      [] + {} == "[object Object]"
      "   " == 0
    \end{lstlisting}

    While these simple cases are unusual the complexity they cause in large programs can be quite substantial. Often, a value might be implicitly coerced between types without a programmer expecting it to have happened. This error might not be noticed until a non-existent method is called on it (for example, attempting to append numbers instead of strings), or alternatively it could be serialised and sent to a server, where it's rejected for having the wrong type.

    Regardless of what happens, the error will be detected at a line of code totally unrelated to the location of the coercion which caused it. The added complexity of finding the underlying bug can be substantial, especially when third-party libraries are involved, since they have their own semantics and untyped APIs.

    Despite having some quirks though, the core of JavaScript is simple enough to express some powerful programs. Douglas Crockford writes in “JavaScript: The Good Parts”:

    \begin{quote}
      In JavaScript, there is a beautiful, elegant, highly expressive language that is buried under a steaming pile of good intentions and blunders. The best nature of JavaScript is so effectively hidden that for many years the prevailing opinion of JavaScript was that it was an unsightly, incompetent toy. My intention here is to expose the goodness in JavaScript, an out- standing, dynamic programming language. JavaScript is a block of marble, and I chip away the features that are not beautiful until the language’s true nature reveals itself. I believe that the elegant subset I carved out is vastly superior to the language as a whole, being more reliable, readable, and maintainable.
    \end{quote}
    \autocite{Crockford2008}

  \section{Existing Implementations}
    One consequence of JavaScript's status as the de-facto language of the front-end web is that until recently there's been no easy way out of using it, and as such tools have developed to mitigate the risk of introducing errors that aren't caught at deployment time. These range from supplementary tools like linters and testing frameworks, through to bigger solutions such as new programming languages and so called transpilers that compile other languages to JavaScript.

  \subsection{Code Quality Tools}

    Starting with the simpler cases, the tools with the lowest overhead for programmers to add to their projects are usually linters. These exist for many programming languages (with the original Lint program being for C), and the JavaScript implementations are JSLint or JSHint. Both perform the same purpose - to alert the programmer to syntax errors or stylistic mistakes - though JSHint is often considered less opinionated and dogmatic. Neither have any type inference capabilities, and they solely focus on the syntactic structure of the program. As such, they're a valuable tool for programmers and the were the first code analysis tool for JavaScript, but aren't comparable to what this project aims to achieve.

  \subsection{Transpilers}

    A recent phenomenon is the use of Emscripten\autocite{Zakai} to compile existing languages from LLVM bytecode into JavaScript. Emscripten is a backend for LLVM which takes the bytecode created by compilers for the language, and converts it into JavaScript, using web API calls where native ones would usually be used. One particularly impressive instance of this is recompiling the native desktop game Quake into a browser game, which was demonstrated by Mozilla at a conference in 2013\autocite{unrealengine}.

  \subsection{TypeScript}

    Another potential solution was created by Microsoft in 2012. TypeScript is a strict superset of JavaScript that supports optional typing. In addition, it adds multiple language features that aren't natively present in JavaScript - classes, interfaces and enums, to name a few. In doing so it does begin to drift away from JavaScript's own semantics and into a new domain, however for some developers this could be an acceptable decision to make.

    It's likely, though, that this would not be an easy jump to make for developers of an existing or legacy piece of software. In particular, the addition of classes to TypeScript means it has a semantic separation between record types with well-typed properties, and objects (which in JavaScript are simply string-expression hashmaps). As such, an array of JavaScript-style objects with varying properties does not have the same quality of type inference as an array of class instances.

    To highlight a specific problem, the expression:
    \begin{lstlisting}
      [{'foo': 3, 'bar': 3}, {'foo': 2, 'baz': 3}]
    \end{lstlisting}
    has the union type
    \begin{lstlisting}
      ( {'foo': number, 'bar': number}
       |{'foo': number, 'baz': number})[]
    \end{lstlisting}

    TypeScript can see that it's an array of inferable types, but can't go as far as to see what properties those objects have in common.

    As such, any program which looped through that array while operating on the \texttt {bar} property would cause problems at runtime during the cases where it's undefined, and these problems would not be caught at compile time.

    Another problematic case is of function inference. TypeScript will never infer any type signatures of functions beyond giving them a default parameter type of \texttt{any}. This means most of the code that developers write will gain no benefit from begin run through TypeScript, and to actually gain any type safety, developers would have to annotate function parameters as required by TypeScript, which would break compatability with the JavaScript syntax.

    These cases highlight a general pattern in which reaping the benefits of the type safety which TypeScript provides requires you to invest fully in its own language, which is a jump existing software may not be able to make.

  \subsection{Flow}
    Flow is the closest comparison to this project that's presently publicly available. While it's still young - it was first announced less than 6 months before the initial research for this project took place - it has a similar purpose and a substantial effort behind it in the form of Facebook.

    Flow was created by Facebook to accompany their increasing number of tools and libraries for frontend JavaScript. It's a static type checking tool for JavaSript, which in principle supports many of the objectives of this project. However, at times it has similarities with TypeScript - in particular, it uses Union types in cases where other type schemes might use polymorphism, and so has a number of cases where type errors may be missed.

    One example of where flow's use of union types causes false negatives to be given can be seen in the inference of expressions and structures which involve, among other language features, the overloaded + operator. This operator is a problematic addition to the language which readers will see regularly used as a yard stick for testing the quality of a type checking implementation. As will be mentioned in the design phase, + is valid for Numbers and Strings, but causes type coercion when used on a mixed pair of subexpressions. Flow infers the types of expressions involved in a + operation as being \texttt{number | string}, and as such provides no security against \texttt{2 + "3"} as both sides of the expression satisfy the type constraint, which fails to express that both subexpressions must be of the same type.

    Not to mention, it's also a complicated tool to configure, and does encourage changing the source code of input programs to add annotations either in the source itself (making it incompatable with standard javascript) or in the comments (which can easily become outdated and incorrect if a function is refactored without updating the comments).

    Most frustratingly, though, it's complicated to run and configure for a JavaScript project. It requires complex configuration files to be created, and each source file must be updated with an initial comment indicating it should be checked by flow, otherwise it would be skipped by default.

  \section{Previous Research}

    \subsection{Hindley-Milner}
      First presented by Hindley in 1969 and later independently discovered by Milner in 1978, what is now known as the Hindley-Milner Type System is the foundation of ML's type inference algorithm and is a core part of many others\autocite{Hindley1969}. Milners work also introduced Algorithm W, an algorithm for type checking programs conforming to that type system\autocite{Milner1978}. While JavaScript has some specific implementation details that mean it's not similar enough for a direct implementation of Algorithm W to be applicable, it does provide a solid foundation from which a JavaScript-specific type system could be designed.

    \subsection{Gradual Typing}
      Outside of statically type checked languages, much research has been done for the checking of programs written in dynamically typed languages.

      Quasi-static typing\autocite{Thatte1989} is one approach in which a top type $\Omega$ is defined, representing the dynamic type. However, Siek \& Taha later observed that as rules exist within this type system which allow the up-casting of ground types to $\Omega$, as well as rules which allow down-casting of $\Omega$ to ground types. These rules can be used to show that some programs which are obviously incorrect to the human reader can type check without error because an incorrect type is cast up to $\Omega$ and back down to the correct type\autocite{Siek2006}.

      That observation was made in the presentation of type checking research for dynamic functional languages. Siek \& Taha proposed a gradually typed lambda calculus along with a full type system and inference algorithm for it\autocite{Siek2006}. This research, in addition to research into Gradual Typing for Objects\autocite{Siek2007} provide a solid backdrop of research from which an implementation for JavaScript could be derived.


\chapter{Specification}

  ``the structure of a type may be partially known/unknown at compile- time and the job of the type system is to catch incompatibilities between the known parts of types.''\autocite{Siek2006} is one which this project should pay attention to.

  [] - consistency

  [] - clarity

  ???

\chapter{Analysis}

  \section{Challenging parts of JavaScript}

    In order to determine what elements of prior type checking research could be applied to JavaScript, we should consider what the hardest parts of the language would be to type check. These can then be borne in mind when deciding which approaches to use - thosse approaches which can either handle them already, or could be slgihtly adjusted to handle them, are preferable to those which would need more significant amendments.

  \subsection{Higher-Kinded types}
    While not given this name in any JavaScript literature, in other programming languages the idea of type constructors and higher-kinded types is well recognised. JavaScript only has three types that could be viewed this way - the Function, Number and Object types.

    The primitive types are Number (a general type for positive and negative floats and ints, and also Infinity and -Infinity), String, Boolean, Regular Expression, Undefined and Null.

    Below are some example Functions, annotated with what could be considered to be their `type'.

    \begin{lstlisting}
      function () {};                   () -> Undefined
      function (a, b, c) {};            (Any, Any, Any) -> Undefined
      function (a, b, c) {return 3;};   (Any, Any, Any) -> Number
      function (a, b) {return a - b};   (Number, Number) -> Number
    \end{lstlisting}

    Additionally functions are first class, and can either be named or anonymously assigned to a variable:

    \begin{lstlisting}
      var add = function (a, b) {return a + b}; var b = add(1, 2);
      function add (a, b) {return a + b}; var b = add(1, 2);
    \end{lstlisting}

    These cases are identical in terms of their behaviour and how they're treated by an interpreter. An obvious extension of the first class nature of functions is that it's also totally acceptable for functions to return other functions:

    \begin{lstlisting}
      var add = function (x, y) {return function(z) { return x + y + z }}
    \end{lstlisting}

    Some Objects, which are simply string-value maps in JavaScript:

    \begin{lstlisting}
      {a: 1}                            Object<a -> Number>
      {a: 'Foo', b: false}              Object<a -> String, b -> Boolean>
    \end{lstlisting}

    And some Arrays:

    \begin{lstlisting}
      [1, 2, 3, 4, 5]                   Array<Number>
      ['1', '2', '3', '4', '5']         Array<String>
    \end{lstlisting}

    More challenging is the case of determining what to do in these cases:

    \begin{lstlisting}
      [1, false, '3']
    \end{lstlisting}

    Where a common general type for all of the array members could not be found. Or similarly, this case:

    \begin{lstlisting}
      [{a: true}, {a: 5}]
    \end{lstlisting}

    Serious thought must be given to determining how to unify two objects with identical member names but different types for those members.

  \subsection{Arbitrary-Length functions}
    JavaScript functions can have any number of parameters, in contrast to lambda calculus and ML, where a function would take one parameter and return a function which takes a second (Or alternatively, would take a tuple of n parameters). Function calls can even supply a different number of parameters than a function is defined to take. Surplus parameters ignored, while unsupplied parameters default to \texttt {undefined} For example, this function:

    \begin{lstlisting}
      var add = function(a, b) { return a + b; };
    \end{lstlisting}

    Could be called with \texttt { add(1, 2, 500) } which would just return 3. Alternatively, calling \texttt {add ("") } would return \texttt {"undefined"}, as an empty string + undefined results in the literal word undefined.

    Special consideration should be given to how to represent functions, and whether calls which supply a different number of parameters should be considered type-correct at all.

  \subsection{Operator Overloading}
    One significant cause of confusion in JavaScript is the overloading of the $+$ operator for Strings and Numbers. While working with strings, it serves as a method of concatenation, and with numbers, it carries out addition. While this doesn't outright cause any errors in consistency, it does lead to cases where an inferred type can be ambigious. The simplest case highlighting this ambiguity is an addition function - \texttt {function (a, b) { return a + b }}.

    Different languages handle this in different ways. In ML and derivatives, the problem is entirely avoided as + is defined for Numbers alone. In Haskell, the function would have the type \texttt {Num a => a -> a -> a} - that is, the function is polymorphic over type \texttt{a}, so long as \texttt{a} implements the behaviour of the \texttt{Num} typeclass.

    Whatever solution is decided for this, the inferred type of the add function should express that numbers are accepted, and strings are accepted, but both parameters must be of the same type.

  \subsection{Implicit Type Coercion}
    A number of examples of implicit coercion were listed in the introduction, for convenience here they are again:

    \begin{lstlisting}[language=JavaScript]
      0 + "" == "0"
      "0" - 1 == -1
      "" - "" == 0
      {} + [] == 0
      [] + {} == "[object Object]" // addition not commutative
      "   " == 0
    \end{lstlisting}

    These type coercions usually occur when two values are operated upon by a single operator. In theory, type checking this should be achieveable by confirming firstly that the two values are of a type acceptable for the operator, an also are of the same type of each other. This would mean that \texttt{2 + 1} type checks, as does \texttt{"2" + "1"}, however \texttt{2 + "1"} does not.

  \subsection{Reassignment}

    A notable feature present in JavaScript but not in functional languages is that of reassignment - while JS has a functional flair, it still supports imperative programming, and with it, mutable state. Variables can be declared and reassigned to values with totally different types. Variables can even be declared with no value, left with the default value \texttt{undefined}, and only assigned a value later. These programs are totally correct JavaScript and don't necessarily cause type errors - how they are handled should be given specific consideration. A number of cases include

    Delayed assignment:
    \begin{lstlisting}
      var x; x = 1;
    \end{lstlisting}

    Reassignment to different type:
    \begin{lstlisting}
      var x = 1; x = false;
    \end{lstlisting}

    Reassignment to more specific type:
    \begin{lstlisting}
      var x = [{a: 5, b: 2}]; x = [{a: 2}];
    \end{lstlisting}

    Or similarly:
    \begin{lstlisting}
      var x = function(a, b) {return a};
      x = function(a, b) {return b};
    \end{lstlisting}

  \subsubsection{Summary}

    Each of these are instances of valid operations within JavaScripts type system, and as such are likely to be used in real world programs. Care should be taken when considering a design for a solution to ensure that these cases are handled correctly. Ahead in the design section, a solution is proposed that meets these requirements with acceptable of programs that type check as incorrect.

  \section{Approach}

    Based off existing research there are 2 potential approaches to performing type inference. One would be to make direct use of the work acheived in Gradual Typing for Functional Languages\autocite{Siek2006} by reducing JavaScript programs down to a gradually typed lamda calculus, and performing the type checking on that lambda calculus. The other would be to perform type checking closer to that described in  Hindley \& Milner's work, by inferring types directly from the structure of a program using a formalised type system. Neither of these approaches are 1:1 implementable, but each approach has its own challenges.

  \subsection{Gradually Typed Lambda Calculus}

    Reducing JavaScript to a Gradually Typed Lambda Calculus in particular would be difficult, as there are features of the language which are not represented in the calculus they describe. Those include significant features to the language such as assignment and reassignment, the presence of the overloaded $+$ operator which works for JavaScript Strings and Numbers, and also arbitrary-size function parameters. While these challenges are not necessarily impossible to overcome, they do come at the added cost of another intermediate representation of the program being checked.

    Another intermediate representation means another potential case of mistranslation of source code, and also another layer of complexity in connecting expressions undergoing type checking to the line-and-column where they appear in the source. While this complexity may be worthwile if the representation was already complete, in this case we would be converting to another intermediate representation to which we would have to define significant extensions, which seems unnecessary.

  \subsection{Hindley Milner}
  \label{subsec:Hindley Milner}

    Though Hindley-Milner Type Inference is primarily used in statically typed languages, (and strictly functional ones too), it brought a number of important approaches to the problem of type checking. One is the distinction between the type system and the algorithm for checking types, and the other is the mapping of type inference onto the language's syntactic structure.

    Some of the simpler HM rules are directly transferrable. In particular, the rule for variable type is usable\footnote{In this notation, $A$ represents an enviroment of assumptions, $e$ represents expressions, $x$ represents variables and $\sigma$ represents type schemes. Additionally, $A_x$ means an environment with no assumptions about $x$, and a type scheme is defined as $\sigma = \tau | \forall\alpha\mspace{3mu}\tau$}:

    \begin{flalign*}
      &&\Cen{3}{
          \textsc{taut:}
          \frac{}{A\vdash x\hmcolon\sigma}
          (x\hmcolon\sigma\in A)
        } &&&
    \end{flalign*}

    as well as rules for instantiation and generalisation:

    \begin{flalign*}
      &&\Cen{3}{
          \textsc{inst:}
          \frac{A\vdash e\hmcolon\sigma}{A\vdash e\hmcolon\sigma '}
          (\sigma > \sigma ')
        } &&&
      \\
      &&\\
      &&\Cen{3}{
          \textsc{gen:}
          \frac{A\vdash e\hmcolon\sigma}{A\vdash e\hmcolon\forall\alpha\text{  } \sigma}
          (\alpha \text{ not free in } A)
        } &&&
    \end{flalign*}

    with some modification to support arbitrary parameter numbers, the abstraction and application rules would also be usable (original rules shown here):

    \begin{flalign*}
      &&\Cen{3}{
          \textsc{abs:}
          \frac{A_{x} \cup \{ x\hmcolon \tau ' \} \vdash e\hmcolon\tau}{A\vdash (\lambda x .e )\hmcolon \tau ' \rightarrow \tau}
        } &&&
      \\
      &&\\
      &&\Cen{3}{
          \textsc{app:}
          \frac{A\vdash e\hmcolon\tau '\rightarrow\tau\mspace{15mu}A\vdash e'\hmcolon\tau}{A\vdash (e\mspace{4mu}e')\hmcolon\tau}
        } &&&
    \end{flalign*}

    however the $let..in$ rule would need some consideration:

    \begin{flalign*}
      &&\Cen{3}{
          \textsc{let:}
          \frac{A\vdash e\hmcolon\sigma\mspace{15mu}A_x \cup \{x\hmcolon\sigma\}\vdash e'\hmcolon\tau}{A\vdash (\texttt{let } x \texttt{ = } e \texttt{ in } e')\hmcolon\tau}
        } &&&
    \end{flalign*}

    The Let rule defines the type of the declared variable within the scope of the $in$ expression, but in JavaScript the equivalent scope would be the scope of the whole function where the assignment took place as the variable can be referenced in later statements in the function. This is one case where the semantic difference between functional and imperative languages becomes important to acknowledge.

    The above rules were taken from Principal type-schemes for functional programs\autocite{Damas1982}. Ahead in the design section is a modified version of these rules which supports features specific to JavaScript, as well as an updated let rule for mutable assignment.

\chapter{Design}

  \section{Formalised Type system}

    To be able to design a system that can correctly check for type errors in JavaScript, we must first begin by laying out what the components of the type system are. Lets first remind ourselves of the ML-style language that Hindley-Milner type inference was designed for. That language, as taken from Principal type-schemes for functional programs \autocite{Damas1982}:
    \begin{flalign*}
      e~::=~x~|~e~e'~|~\lambda~x.e~|~\texttt{let}~x~=~e~\texttt{in}~e'&&
    \end{flalign*}

    This language doesn't have any features of imperative or object oriented programs - including no assignment, flow control or property access - so our type system rules will need to account for the features JavaScript has.

    \subsection{JavaScript Types}

    We begin by formalising the types available to our system:
    \begin{flalign*}
        \gamma~&::=~Number~|~String~|~Boolean~|~RegExp~|~Undefined~|~Null&&\\
        \tau~&::=~\gamma~|~Function<\tau_{0..n},~\tau>~|~Array<\tau>~|~Object<(key\times\tau)_{0..n}>&&\\
        \sigma~&::=~\tau~|~\{\tau_{1..n}\}~|~Any&&
    \end{flalign*}

    Using this type system we can see that we can already overcome a failing of TypeScript and Flow - where they use Union types and lose fidelity, we can the Set variant of our $\sigma$ type scheme. Specifically, if we revisit the idea of the polymorphic add function:

    \begin{lstlisting}
      (function (a, b){ return a + b})(1, '2')
    \end{lstlisting}

    In TypeScript, the above wrongly type checks as correct because a and b have the \texttt{any} type. This is because we didn't manually annotate those parameters with our own type.

    In Flow, it wrongly type checks as correct because the inferred type of the function is too vague. It says the \texttt{a} and \texttt{b} parameters are of the union type \texttt{number | string}, which is partially correct. But what it doesn't express is that though the parameters can be of \textit{either} type, they must be of the \textit{same} type. And, in turn, the return value will also be of that type.

    Our type system can express clearly that this is incorrect - the Function has a set of potential types. We can describe it as:
    \begin{flalign*}
        \{~Function<\{String,String\}, String>, Function<\{Number,Number\}, Number> \}
    \end{flalign*}

    That is, the function has 2 type signatures, both of which are valid and callable. The first is that it takes 2 strings and returns a string, and the second is that it takes 2 numbers and returns a number. However, and importantly, there is no variant of this which allows taking a number and a string. That means the anonymous call above can never type check as correct for this expression.

    This is just one case where our approach is superior to existing tools, as Flow (which is by far the leading comparable tool) fails to detect type errors due to it's relying on union types.

    \subsection{Language Structure}

    The type system alone is not enough to define the behaviour of our system. We additionally need to define the structure of the language, here substantially simplified to the core structures that affect the type system:
    \begin{flalign*}
        op~::=&~e_1~\textsc{binop}~e_2~|~\textsc{unaryop}~e\\
        e~::=&~x~|~[e_0, e_n]~|~\{k_0: e_0,~k_n: e_n\}~|~\texttt{function}(x_{0..n})~\{~js_{0..n}~\}\\
        &~|~e(e_{0..n})~|~e\texttt{.} p~|~e_1[e_2]~|~op\\
        js~::=&~e~|~\texttt{var}~x~|~\texttt{var}~x~=~e~|~x~=~e~|~\texttt{return}~e
    \end{flalign*}

    And given those structures, we additionally need to formulate the rules for type inference within that system. those rules are displayed overleaf, kept together for the sake of legibility.

    \newpage

    \subsection{Type Inference Rules}

    \begin{flalign*}
      \textsc{variable:}&\mspace{20mu}
      \frac{}{A\vdash x\hmcolon\sigma}
      \mspace{20mu}
      (x\hmcolon\sigma\in A)
      \\\\
      \textsc{function:}&\mspace{20mu}
      \frac{A_{x_{0..n}}\cup \{ x_0:t_0, x_n:t_n \} \vdash e:\tau'}{A\vdash \texttt{function}(x_0:\tau_0,.. x_n:\tau_n)\{ \texttt{return}~e:\tau'\}: Function<\tau_{0..n}, \tau'>}
      \\\\
      \textsc{call:}&\mspace{20mu}
      \frac{A \vdash e':Function<\sigma_{0..n}, \sigma'>\mspace{20mu} A\vdash \forall n~(e_n:\tau_n\wedge\tau_n \leqslant \sigma_n)}{A\vdash e'(e_0,..e_n): \sigma'}
      \\\\
      \textsc{define:}&\mspace{20mu}
      \frac{A_x \vdash e:\tau\mspace{20mu} \texttt{var}~x~=~e}{A\vdash x: \tau}
      \\\\
      \textsc{declare:}&\mspace{20mu}
      \frac{A_x \vdash \texttt{var}~x}{A\vdash x: Undefined}
      \\\\
      \textsc{assign:}&\mspace{20mu}
      \frac{A \vdash (x:\sigma \vee x:Undefined)\mspace{20mu} e: \sigma' \mspace{20mu}x~=~e}{A\vdash x: \sigma'}
      \mspace{20mu}
      (\sigma'\leqslant\sigma)
      \\\\
      \textsc{property:}&\mspace{20mu}
      \frac{A \vdash e:Object<y:\tau>}{A\vdash e\texttt{.}y:\tau}
      \\\\
      \textsc{element:}&\mspace{20mu}
      \frac{A \vdash e:Array<\tau>\mspace{20mu} e':Number}{A\vdash e\texttt{[}e'\texttt{]}:\tau}
      \\\\
      \textsc{member:}&\mspace{20mu}
      \frac{A \vdash e:Object<e':\tau>\mspace{20mu} e':String}{A\vdash e\texttt{[}e'\texttt{]}:\tau}
      \\\\
      \textsc{non member:}&\mspace{20mu}
      \frac{A \vdash e:Object<ps>\mspace{20mu} e':String}{A\vdash e\texttt{[}e'\texttt{]}:Any}
      \mspace{20mu}
      (e' \notin ps)
    \end{flalign*}

    The underlying difference between these rules and the rules formulated by Hindley \& Milner is recognisable in the \textsc{define}, \textsc{declare} and \textsc{assign} rules. These are statements which have no type of their own, but help us infer the type of other expressions within a program. Such a concept - of a ``bare statement'' - does not exist in purely functional languages, and as such, when one appears in JavaScript code we use it as part of the premise of a deduction rather than the conclusion.

    \subsection{Unification Rules}

    What remains is to define the unification of types. Below we define an intersection function which takes 2 types and produces the a type which is the intersection of the two of them. Those cases not covered here can be considered to return a type error.

    \begin{flalign*}
      \gamma&\cap\gamma~&=&~\gamma\\
      Object<ps_1>&\cap~Object<ps_2>~&=&~Object<ps_1\cap ps_2>\\
      Array<\sigma_1>&\cap~Array<\sigma_2>~&=&~Array<\sigma_1 \cap \sigma_2>\\
      Function<ps_{0..n}, \sigma_1>&\cap~Function<qs_{0..n}, \sigma_2>~&=&~Function<zipWith(\cap, ps, qs),\\
      & & &~~~~~~~~~~~~~~~~~~\sigma_1\cap\sigma_2> & \\
      Any&\cap~other~&=&~other\\
      \{\sigma_1,..\sigma_n\}&\cap~\sigma_n~&=&~\sigma_n\\
    \end{flalign*}

    In words, this system determines the unification of two types to be the most general type that can satisfy the constraints of both input types.

    For ground types ($\gamma$) that is either the ground type itself when unifying the same ground type from 2 expressions, or a Type Error.

    For Objects, it's the intersection of the properties of both objects. Similarly for arrays, it's the intersection of the types each array is composed of.

    For Functions, each of the required parameter types are intersected, along with the return types. Any type errors occuring within either parameter intersections or return type intersection would mean this this intersection also evaluates to a type error.

    The intersection of $Any$ with any other property simply returns the other property being unified, as it's always at least as specific as any (the case where it's the same specificity is $Any \cap Any$).

    And the intersection between a set of types and a constant type occuring within that set is the constant type, as it's the most general type that can satisfy both sides.

  \section{Software Structure}
    The software architecture for this project will align quite closely with the common undertanding of compilers.

    An initial parsing phase will produce an untyped Abstract Syntax Tree. As no type checking or inference will have been performed at this stage, the only information contained within this structure (besides the representation of the program) is the position within the raw source at which each element of the tree occurs. This is necessary to ensure that when a type error is detected, we can give the user a specific reference to where it occurred.

    The major phase of the software's execution is to take the untyped abstract syntax tree and convert it into a typed abstract syntax tree. This new tree will be structured identically to the untyped tree, but will carry extra information about the types (and potentially type errors) of the statements and expressions at each level. It's a simple tree-traversal algorithm, in which any subtrees undergo type inference first, followed by unification by the rules defined in the previous section. In the case of rules relating to variables and scope, data about the types of variables is kept in an environment variable, which is simply a map of variables to their types (ensuring of course that identically named variables in different scopes don't affect each other).

    Finally this typed tree will undergo error checking, looking for the locations within the tree where type errors occur, and feed that back into a number of locations in the source code which can be fed back to the user along with the tool's understanding of the error.

\chapter{Implementation}
  Before implementation began it was necessary to decide which language would be used for the implementation. The decision for which language to use was based on the outcome of implementing small parsers in 2 potential languages, OCaml and Scala. Each implementation parsed a simple subset of JavaScript, roughly aligning with this grammar:
  \begin{flalign*}
    exp~&:=~x~|~num~|~str~|~\texttt{function(}~x_0..x_n~\texttt{) \{}~statement*~\texttt{\}}~|~exp~\texttt{(}~exp_0..exp_n~\texttt{)}\\
    statement~&:=~\texttt{var}~x~\texttt{=}~exp~|~\texttt{var}~x~|~\texttt{return}~exp~|~exp\\
    program~&:=~statement*\\
  \end{flalign*}

  The Ocaml implementation made use of Menhir, an LR parser generator, while the Scala version used an implementation of Parser Combinators\autocite{Hutton1996}, a tool which allows the compositon of smaller parsers into larger ones. Parser combinators often come at a cost of efficiency as backtracking is a core part of the operation of composed parsers, however we made use of a specific implementation based on Packrat Parsing\autocite{Ford2006} which makes heavy use of memoisation and can provide linear time complexity.

  Neither implementation was particularly more challenging than the other, and so the decision of which language to use came down to the quality of the parsing tool and the ease of working with it. The reason behind this is that the bulk of this project involves analysis of ASTs, and it would add no benefit to the outcome to be spending any more time working with a grammar than is necessary.

  From this requirement, the decision was made to work with OCaml, primarily because of the quality of Menhir. It provides a number of features which are lacking in Parser combinators - firstly the ability to separate the precedence of operators from the grammar itself, meaning expressions like $1 + 4 / 2 * 3$ can be parsed correctly without complicating the grammar by splitting expression rules into factors and terms. As it creates LR parsers, Menhir also allows left-recursive grammars, which Parser Combinators do not.

  this turned out to be the wrong decision, resulting in a change to Scala in January. The following sections outline the implementation process for OCaml, the problems that arose causing the switch, and finally the implementation process for Scala.

  \section{OCaml}
    The OCaml implementation consisted of 4 core subsystems, similar to those discussed in the Design section. They are the lexer, parser, inference engine and error checker. Additionally there are 2 modules which define the type system and the abstract syntax tree.

    Notably this implementation (which was the first attempt at writing a type inference tool) does not create a typed syntax tree from the untyped one, instead folding over the tree, starting by inferring types from the leaf nodes, then building back up the tree and increasing the knowledge of the variables in the scope. While this implementation can still correctly identify type errors, it wasn't ideal, as is discussed ahead.

    \subsection{Data Structures}
      For the sake of simplicity, the initial OCaml implementation began just with support for numbers, strings, booleans and the 3 higher kinded types. The type schemes were defined as follows:

      \begin{lstlisting}
        type t =
          | TNumber
          | TString
          | TArray of inferred_type
          | TBoolean
          | TObject of (string * inferred_type) list
          | TFunction of (inferred_type list * inferred_type)
          | TUndefined
          | TNull
        and inferred_type =
          | T of JSTypeSet.t
          | TofV of StringSet.t
          | EUndeclared
          | EConflict of JSTypeSet.t * JSTypeSet.t
        and JSTypeSet : Set.S with type elt = JSType.t = Set.Make(JSType)
        and StringSet : Set.S with type elt = String.t = Set.Make(String)
        module SSMap = Map.Make(StringSet)

        type scope = JSTypeSet.t SSMap.t
      \end{lstlisting}

      In English, a scope is a map of sets of variables which share a type, to the set of types that they share. In otherwords, our polymorphic add function would have a scope of:
      \begin{math}
        \{a, b\} \rightarrow T~\{\texttt{TNumber},~\texttt{TString}\}
      \end{math}

      The abstract syntax tree is defined using a recursively defined pair of types, one for statements and one for expressions, as well as two types created for convenience:

      \begin{lstlisting}
        type fname = string option
        and block = statement list
        and statement =
          | Return of expr
          | Declare of (string * expr option)
          | Assign of (string * expr)
          | While of (expr * block)
          | If of (expr * block)
          | IfElse of (expr * block * block)
          | Expr of expr
        and expr =
          | Add of (expr * expr)
          | Sub of (expr * expr)
          | Mul of (expr * expr)
          | Div of (expr * expr)
          | Mod of (expr * expr)
          | Lt of (expr * expr)
          | LtEq of (expr * expr)
          | Gt of (expr * expr)
          | GtEq of (expr * expr)
          | Eq of (expr * expr)
          | True
          | False
          | Ident of string
          | Number of int
          | String of string
          | Function of (fname * string list * block)
          | Call of (string * expr list)
          | Object of (expr * expr) list
          | Property of (expr * expr)
      \end{lstlisting}

      In this AST structure, a program is simply a list of statements. These structures only represent a subset of JavaScript but are enough to form the basis of the language for the purposes of type inference - they can represent variables, functions, assignment and flow control, which are the core properties we should be able to infer. Other elements of the language (such as while loops or for loops) are simple to add once this subset can be handled correctly.

    \subsection{Inference}
      Knowing that programs are represented as a list of statements, the inference algorithm is implemented as a $fold$ which builds up a scope of types as it passes over each statement. When folding over expressions, the types for leaf nodes within the expression subtree are calculated first before being unified and returned. When folding over a statement, the effect the statement has on the scope is stored in the scope.

      From this point, the program can give a summary of what the types of variables are within the program, including when there are no types that can satisfy the constraints on the variable. However, this is only part of the purpose of the program - it must also be able to infer the types of expressions, and further than that, report the positions within the source code of type errors.

      To reach that point, a number of extensions to the software would have to be made.

      One would be the extension of the parsing process to produce an Abstract Syntax tree containing the locations of each of the tokens present, so their locations can be given back to the user. Additionally, a second, typed AST would need to be constructed which would be able to store information about the types of every expression in the tree, and able to report type errors when they occurred.

      The complexity of implementing that in OCaml is non-trivial, particularly in keeping track of the location of each input character through the tokenisation, parsing and inference stages. It quickly became clear that implementing these additional features in OCaml would come at a substantail time cost, and so the decision was made to switch to Scala which was known to have a more expressive type system.

  \section{Scala}
    Implementation in Scala began in ernest, using the test cases discussed in the testing section as a reference point, as these had already been constructed for OCaml. The first step of implementation was the creation of the typed and untyped AST structures discussed previously, as well as the structures representing the types inferred by the system.

    Scala has a number of features which were particularly useful for the creation of these types. In particular these are case classes, a specific form of classes whose instances support pattern matching, and traits, which are similar to Java Inferfaces but also allow some implementation to be specified. Traits are Scala's main feature for the composition of properties and can be combined with the \texttt{sealed} keyword to indicate that a trait can only be used within one source file. This in turn means the compiler can give hints about when pattern matching against those classes might be inexhaustive.

    \subsection{Untyped AST}
      The AST structure makes heavy use of traits and cases classes, and some elements from every level of the AST are displayed here:
      \begin{lstlisting}
        case class Position(startLine: Int, startCol: Int, endLine: Int, endCol: Int)

        trait Pos {
          val pos: Position
        }

        sealed trait ASTNode extends Pos
        case class Program(statements: Seq[Statement], pos: Position) extends Pos

        sealed trait Statement extends ASTNode
        case class If(cond: Expr, block: Seq[Statement], pos: Position) extends Statement

        sealed trait Expr extends Statement

        sealed trait Op extends Expr
        case class Add(lhs: Expr, rhs: Expr, pos: Position) extends Op

        sealed trait Value extends Expr

        case class Ident(name: String, pos: Position) extends Value
        case class LiteralStr(string: String, pos: Position) extends Value
      \end{lstlisting}

      We start by defining a \texttt{Position} case class, which is needed for the final stage of the process where we highlight type errors to the user. We additionally define a \texttt{Pos} trait, all extensions of which must carry a position. This is useful as we later guarantee through \texttt{ASTNode} that all nodes of the syntax tree must carry a position.

      A \texttt{Program} is defined to simply be a list of statements, which also has a position. While that's not necessary for the purposes of the software, it's helpful from a development perspective to be able to check that the entirety of a program has been parsed in order to validate that the parser doesn't drop any input. More on the validation of the parser later.

      A \texttt{Statement} is the most general case of a component of a program, representing both the untyped structures such as variable declarations and flow control statements, as well as being implemented by \texttt{Expr} to represent all of the typed components of a program.

      The \texttt{Expr} trait has no classes which directly implement it, instead serving as a common type for operations - primitive \texttt{Value}s and more complex \texttt{Op}s, representing operations composed of other expressions.

    \subsection{Parsing}
      As was mentioned previously, the Scala implementation makes use of parser combinators to parse the source code of programs. While these look like standard grammar rules, there were a number of difficulties to overcome. One was a difficulty of parser combinators themselves - their inability to perform left recursion - while the others were a deficiency in the parser library implementation and also of the ambiguity of JavaScript for the purposes of tokenisation.

      \subsubsection{Ambiguous Tokenisation}
        One of the simpler but more frustrating problems during parser construction was the case of parsing JavaScript Regular Expressions correctly. JavaScript has a literal notation for regular expressions, such that anything between two \texttt{/} tokens could be considered a regular expression. This causes serious difficulty for parsers, as there is then ambiguity at the lexing stage as to whether \texttt{x/y/z} is ``(x) (divide) (y) (divide) (z)'', or ``x (regular expression) x''. Solving this issue was simple but came at a slight performance cost - removing the lexing stage from the pipeline so the parser combinators operate solely on the raw source of the input. While it would have a performance impact as the combinator backtracking process would now apply to whitespace as well as just tokens, performance is not the objective of this project and so the slight decrease in performance is an acceptable cost.

      \subsubsection{Positional Information}
        The most pressing feature of the parser to implement having moved from OCaml was to return positional information of the parsed source. Native parser combinators can return the location in source of the beginning of their parsed results, however don't by default expose the position at which the parsed input ends. This information is available within the implementation, however, and just needed to be exposed along with the result of parsing.

         Implicit conversions are a Scala language feature that allow the programmer to define behaviour on some intermediate class, and separately define conversions from existing classes to that class. An implicit class is a specific example of this pattern which allows a programmer to define some method $m$ on a intermediate class $c$, in conjunction with some conversion from type $t\rightarrow c$. This means if a programmer calls the method $m$ on some value of type $t$, Scala will implicitly convert $t$ to $c$ using the implicit class provided by the programmer, if that class is in scope.

        Implicit classes must be unique in their type signature and will cause a compilation error if 2 implicit functions or classes exist between the same types.

        We use an \texttt{implicit class} to augment the behaviour of the standard parser combinator, so as to return a position in the input stream from which the result that has been parsed. It was implemented as follows:

        \begin{lstlisting}
          implicit class PositionedParser[T](parser: Parser[T]) {
            def :>[U](p: ((T, Position)) => U): Parser[U] = Parser { in =>
              parser(in) match {
                case Success(t, in1) => Success(p(t, Position(in.pos.line,
                                                              in.pos.column,
                                                              in1.pos.line,
                                                              in1.pos.column)
                                                 ),
                                                in1
                                               )
                case ns: NoSuccess => ns
              }
            }
          }
        \end{lstlisting}

        The defined \texttt{:>} method simply wraps the parser, taking the position in the stream of both it's input and the remainder of the stream in it's output, and passes a \texttt{Position} constructed from those values to the function which handles it's result.

      \subsubsection{Left Recursion}
        Due to their reliance on backtracking, parser combinators are unable to parse grammars which include any left-recursive rules. This is intuitive when considering that a rule is evaluated by trying it's first component, then if it parses, continuing on to the next, or alternatively returning an error and attempting the next tree. A left recursive rule will continually call itself until a stack overflow is reached.

        To prevent this happening, a general pattern of grammar structure is needed, which is a small deviation from the structure needed to maintain operator precedence. Assuming a small expression language language $e$:

        \begin{flalign*}
          e~::=~n~|~e+e~|e*e
        \end{flalign*}

        In which we need to maintain the operator precedence of keeping $*$ highest and $+$ lowest, we'd structure our Parser as below. Here the syntax \texttt{a \~ b} means compose the parsers \texttt{a} and \texttt{b} such that the result is a parser which matches \texttt{a} followed by \texttt{b} and returns a tuple of their results. \texttt{a \~> b} means compose the parsers in the same way as \texttt{\~} but only return the right hand result. The \texttt{?} operator means the parser is optional, and as such can match no input.

        \begin{lstlisting}
          def exp = mul
          def mul = add ~ ("*" ~> mul)?
          def add = num ~ ("+" ~> add)?
          def num = numericLit
        \end{lstlisting}

        In this system, we avoid left recursion by beginning attempting to parse a high precedence operator, which in turn parses lower precendence operators followed by an optional recursive call to itself. In this way we make the parsers tail recursive, where (for example) if no multiplication operation is present the parser simply returns the result of the lower operators. However, if there is an operator present, it makes a recursive call to itself using the rest of the input to determine the right hand side.

        We introduce an \texttt{implicit class} to our language to make working with the results of tail-recursive parsers easier:
        \begin{lstlisting}
          implicit class LeftRecursiveParser[U, T](parser: Parser[U ~ Option[T]]) {
            def ::>(p: ((U, T, Position)) => U): Parser[U] = parser :> {
              case (e ~ Some(e1), pos) => p((e, e1, pos))
              case (e ~ None, pos) => e
            }
          }
        \end{lstlisting}

        This class allows us to call the \texttt{::>} method on any parser which parses a $U$ followed by an optional $T$. This method allows us to specify an operation to perform when an operator is parsed on the right hand side, and for the cases where no right hand side is parsed, simply return the result unmodified.

        In other words, above, where we defined \texttt{mul = add \~~("*" \~\ > mul)?}, we can now define what we'd like to do with the result of that parser if it does parse a multiplication expression. Our function will run if the expression is parsed, or simply skipped (and the addition returned unchanged) if no expression is present.

        A fuller example of our expression parser would look like this:
        \begin{lstlisting}
          def exp = mul
          def mul = add ~ ("*" ~> mul)? ::> { case (lhs, rhs, pos) => Mul(lhs, rhs, pos)}
          def add = num ~ ("+" ~> add)? ::> { case (lhs, rhs, pos) => Add(lhs, rhs, pos)}
          def num = numericLit
        \end{lstlisting}
        This parses each level of the tree, and when operations are parsed, creates an AST node containing the subtrees and their respective positions in the raw input.

    \subsection{Data Structures}
      Our Type System is defined using case classes for the higher-kinded types, and \texttt{case object}s for the primitives, which are essentially singleton instances of case classes. This obviously mirrors the fact that there is only one \texttt{TNumber} type, but several implementations of Objects, Functions, and Arrays do exist. A short extract of their definitions are as follows:
      \begin{lstlisting}
  sealed trait JsType
  case object TNumber extends JsType
  case object TRegExp extends JsType
  case object TString extends JsType
  case object TBoolean extends JsType
  case object TUndefined extends JsType
  case object TNull extends JsType

  case class TFunction(params: Seq[InferredType], result: InferredType) extends JsType {
    def applyCall(ps: Seq[Expr])(implicit st: ScopeStack): InferredType
  }

  case class TObject(
    override val properties: collection.mutable.Map[String, InferredType]
  ) extends JsType

  case class TArray(t: InferredType) extends JsType
      \end{lstlisting}

      This is somewhat simplified as there are additional methods defined on the higher kinded types beyond the method for function application on the TFunction class (the implementation is omitted here for brevity).

      The type referenced in the higher kinded types, \texttt{InferredType}, is defined below:

      \begin{lstlisting}
  sealed trait InferredType {
    def intersect(other: InferredType)(implicit st: ScopeStack): InferredType
    def canSatisfy(other: InferredType)(implicit st: ScopeStack): Boolean
    def applyCall(ps: Seq[Expr])(implicit st: ScopeStack): InferredType
  }

  sealed trait DirectType extends InferredType

  case object AnyT extends InferredType with DirectType
  case class ConstT(t: JsType) extends InferredType with DirectType
  case class SetT(ts: Set[JsType]) extends InferredType with DirectType
  case class IntersectT(vs: Int) extends InferredType

  sealed trait TypeError extends InferredType with DirectType
  case class NoInterErr(possibles: Set[InferredType]) extends TypeError
  case class BadArgsErr(f: TFunction, e: Seq[InferredType]) extends TypeError
  case class NotAProperty(obj: TObject, property: String) extends TypeError
  case class NotAssignableErr(assignee: InferredType, value: InferredType) extends TypeError
      \end{lstlisting}

      It's worth enumerating the purpose and definition of each of these types. \texttt{AnyT} has 2 similar but subtly different meanings in different context, one when it's the type of a value, and one when it's the type of a parameter in a function.

      As the type of a value, \texttt{AnyT} means the value could be any type, and we're unable to perform type checking on any activities that are achieved with it. It's not that the type is undefined, it's that it can not be defined statically (or at least, our tool isn't advanced enough to infer what's happening). In this case, if a method is called on it, or it's appended to an array, we should stick to our specification and trust the programmer's judgement.

      When seen as the parameter type to a function, \texttt{AnyT} means a value of any type can be supplied to this function. In a Hindley Milner type system, the equivalent would be $\forall\alpha~\alpha$.

      The type \texttt{ConstT} represents a type which is exactly known. This could be inferred from the type of a literal value, for example, where there is no doubt as to what the type might be. This is in contrast to \texttt{SetT} which represents a finite set of types which are valid for the expression.

      We also have the trait \texttt{TypeError} which implements InferredType in order to allow expressions with Type Error to express that in their type.

      The remaining class under the \texttt{InferredType} trait is the \texttt{IntersectT} trait. This is best understood in the context of \texttt{ScopeStack}, a class also referenced in the type signatures of some \texttt{InferredType} methods. An \texttt{IntersectT} can be thought of as a pointer to a type contained within a \texttt{ScopeStack}, which is used to keep track of the scopes of closures, their parameters and return types. It's this technique that allows us to link the dependency between parameter types; our overloaded add function has the type \texttt{ConstT(Function<[IntersectT( n), IntersectT(n)], IntersectT(n))} where \texttt{n} is defined within the \texttt{ScopeStack} as having the type \texttt{SetT(TNumber, TString)}.

      This is a small deviation from our original plan of assigning the type \texttt{SetT(Function< [TNumber, TNumber], TNumber>, Function<[TString, TString], TString)} - however this new approach is functionally identical (there's no expression that could be expressed in the \texttt{Set} design that can't be expressed with the \texttt{IntersectT} design) and we additionally gain clarity from separating the logic of which variables share types, and what those types are.

      The only remaining data structure to discuss is the Typed syntax tree. This is almost identical to the untyped syntax tree but with two small exceptions, visible from the extracts of type definitions:
      \begin{lstlisting}
  type TypeLines = Seq[(Position, InferredType)]
  sealed trait ASTNode extends Pos {
    def typeAnnotations(m: Map[Int, TypeLines]): Map[Int, TypeLines]
  }
  sealed trait Expr extends Statement {
    val t: InferredType
  }
  case class LiteralStr(
    string: String,
    pos: Position,
    t: InferredType = ConstT(TString)
  ) extends Value
      \end{lstlisting}

      The definition of the \texttt{typeAnnotations} method on our AST Node class is useful for the output of the program. It is defined so that at the end of the process, when annotating type errors, the program can retrieve from each node a list of line numbers in the source code, and from each one get a list of the positions of expressions, and the types of those expressions. At the end of the program's execution, this map is calculated for the entire tree to allow looping through each line and annotating every expression on that line with its type.

      Additionally, the definition of the \texttt{t} property on expression nodes. This is present for all values and operations, and is populated with the type calculated by the type inference algorithm.

      And finally, we save complexity elsewhere in the process by defining the types of primitive types in their case class definition, as seen when we assign all literal string nodes the default type of \texttt{ConstT(TString)}.

      \subsection{Inference Algorithm}

      The process of turning an untyped abstract syntax tree into a typed one is based from defining specific conversions for each kind of node between it's typed and untyped one. In the case of primitives, these are simple:

      \begin{lstlisting}
        case AST.LiteralStr(s, pos) => ASTf.LiteralStr(s, pos)
      \end{lstlisting}

      Recall that the typed form of primitive node definitions have their type included in the definition, so no type declaration has to happen for these simple cases.

      For the more complex ones, we simply take the types of subtrees, and use their intersection as the type:

      \begin{lstlisting}
        case AST.Add(lhs, rhs, pos) => add(lhs, rhs, pos)
      \end{lstlisting}

      where our utility add method is defined as follows:

      \begin{lstlisting}
        def add(lhs: Expr, rhs: Expr, pos: Position)(implicit st: ScopeStack) =
            Add(lhs, rhs, pos, lhs.t intersect rhs.t intersect SetT(Set(TNumber, TString)))
      \end{lstlisting}

      This is a slight simplification of this method implementation; another helper method is involved which provides more generalised inference for binary operations, but this is the constructed value none the less.

      In this way, our inference algorithm works by starting at the bottom of the tree and unifying types as it returns too the root, in accordance with the unification rules we laid out in the design section.

      An extract of the implementation of those rules in Scala:

      \begin{lstlisting}
def intersect(other: InferredType)(implicit st: ScopeStack): InferredType = {
  this match {
    case AnyT           => other
    case t: TypeError   => this
    case ConstT(t)      => other match {
      case ConstT(u)    => (t, u) match {
                             case (t1: TObject, t2: TObject) => t1 intersect t2
                             case _ if t == u => ConstT(t)
                             case _ => NoInterErr(Set(ConstT(t), ConstT(u)))
                           }

      case SetT(ts)       =>  if (ts contains t)         ConstT(t) else NoInterErr(Set(SetT(ts), ConstT(t)))
      case IntersectT(i)  =>  if ((this canSatisfy other) || (other canSatisfy this)) {
                                st.filterGroupType(GroupID(i), ConstT(t))
                              } else {
                                NoInterErr(Set(ConstT(t), st.getGroupType(GroupID(i))))
                              }
      case _              => other intersect this
    }
    case SetT(ts)       => other match {
      case SetT(us)       => if ((ts intersect us).nonEmpty) { SetT(ts intersect us) } else { NoInterErr(Set(SetT(ts), SetT(us)))}
      case IntersectT(i)  => st.filterGroupType(GroupID(i), SetT(ts)); other
      case _              => other intersect this
    }
    case IntersectT(i)  => other match {
      case IntersectT(v)  => IntersectT(st.mergeGroupTypes(Set(GroupID(i), GroupID(v))).id)
      case _              => other intersect this
    }
  }
}
      \end{lstlisting}

      In the our testing section we go on to prove that this intersection strategy is in line with the rules we defined in our design section using ScalaCheck, a property based testing library for Scala.

      \subsection{Feedback}

      For our system to be of any practical use, it needs to give meaningful feedback to developers about the nature of type errors detected within their programs. The primary approach to this, the \texttt{typeAnnotations} method defined on each tree node is the primary method for retrieving the type information to be displayed on screen. The method for printing this information on screen is pleasantly simple:

      \begin{lstlisting}
      def annotateSource(source: Seq[String]): String = {
        source.zipWithIndex.flatMap({
          case (line, idx) => typeAnnotations(idx + 1).flatMap {
            case (pos, t) => Seq(line, pos.underline, pos.paddedPrefix(t.serialize(stack)) + "\n")
          }
        }).mkString("\n")
      }
      \end{lstlisting}

      We zip every line of the input program together with it's index (which is line number - 1), find every type annotation for that line and the position which it covers, and re-print the input line with the expression underlined, and its type printed beneath it. Some sample IO (here executed with the verbose flag to show all types annotated, for the sake of demonstration).

      \begin{lstlisting}
      > var box = { width: 5, height: 5 };
      > var area = function(w, h) { return w * h };
      > console.log(area(box.width, box.height));

      var box = { width: 5, height: 5 };
                                    ^^
                                    Number

      var box = { width: 5, height: 5 };
                         ^
                         Number

      var box = { width: 5, height: 5 };
                ^^^^^^^^^^^^^^^^^^^^^^^
                Object { height: Number, width: Number }

      var area = function(w, h) { return w * h };
                                             ^^
                                             Number

      var area = function(w, h) { return w * h };
                                         ^^
                                         Number

      var area = function(w, h) { return w * h };
                                         ^^^^^^
                                         Number

      var area = function(w, h) { return w * h };
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 Function (Number, Number) -> Number

      console.log(area(box.width, box.height));
                                  ^^^^^^^^^^
                                  Number

      console.log(area(box.width, box.height));
                       ^^^^^^^^^
                       Number

      console.log(area(box.width, box.height));
                  ^^^^
                  Function (Number, Number) -> Number

      console.log(area(box.width, box.height));
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^
                  Number

      console.log(area(box.width, box.height));
      ^^^^^^^^^^^
      Function (Any) -> Undefined

      console.log(area(box.width, box.height));
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      Undefined

      No type errors found
      \end{lstlisting}

      And here's how our function parses and presents type errors found within a type-incorrect program (here running without the verbose flag):

      \begin{lstlisting}
        > var add = function(a, b) { return a + b };
        > add(1, 2);
        > add("a", "b");
        > add(1, "b");


      \end{lstlisting}

      Also note that the errors are reported to the Standard Error stream, and the system exits with a non-zero exit code when an error occurs. Using a standard interface like this one to report errors rather than reporting errors through some specific or ad-hoc format makes it easier to integrate the tool into existing build processes.

\chapter{Testing}
  Testing of the system is comprehensive, covering the system as a whole with integration tests, and covering some of the formally defined components with fuzz testing.

  \section{Test Driven Development}
    As was mentioned in the implementation section, TDD was used heavily to drive the development process. This process became especially useful after the move from OCaml to Scala, where it was necessary to ensure that no regressions had been introduced into the code. Over time test cases were introduced like these:

    \begin{lstlisting}
      Seq(
        true  -> "1+2+3+4;",
        true  -> "var a = true; a = false",
        true  -> "2*2*3/4+10;",
        true  -> "100;",
        true  -> "'hello ' + 'world';",
      ...
    \end{lstlisting}

    Noting that \texttt{->} is Scala syntactic sugar for a tuple, these cases represent whether a test should type check correctly tupled with the source code of the test case.

    The simple cases above test individual features of the type system, but go on to test several in composition together:

    \begin{lstlisting}
  false -> "var x = [{name: 'Jack', age: 22}, {make: 'Citroen C1', age: 6}]; var i = 0; while (x.length < i++) { console.log(x[i].name); }"
    \end{lstlisting}

    The use of these tests ensured that as development progressed to add more features to the system, there were never any regressions introduced.

  \section{Parser Testing}
    Testing the completeness of the parser was undertaken to ensure that it would correctly be able to parse all valid JavaScript programs, even if it could not perform type inference on them

    The ideal situation for testing would have been some flavour of property-based testing or fuzz testing as was used for testing the Unification system, which is described ahead. However the use of parser combinators blurs the line between the idea of a grammar and a parser, and as such fuzz testing becomes non trivial.

    Instead, testing was performed by finding substantially large open source JavaScript libraries and using them as inputs to the program.

  \section{Type System}
    \subsection{Inference}
    \subsection{Unification}

\chapter{Evaluation}
\chapter{Conclusion}

\printbibliography
\end{document}
